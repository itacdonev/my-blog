[
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Quick Programming Commands",
    "section": "",
    "text": "create index.qdm or index.ipynb with the following upper file info\nadd image in the folder and specify in the post file info else, the first image in the post will be used\n\nhttps://ivelasq.quarto.pub/building-a-blog-with-quarto/lets-build-a-quarto-blog/#about.qmd-blog-about-page https://ivelasq.rbind.io\nEnvironment:\nOnce the post is written run the following: - adjust toc in _quarto.yml - quarto render - quarto publish gh-pages\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Posts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe AI Muse: Navigating the Complexities of Creativity in the Digital Age\n\n\nBook Review: The Creativity Code by Marcus du Sautoy\n\n\n\nbook review\n\n\n\n\nFebruary 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnpacking the Implications of Algorithms on Human Life\n\n\nBook Review: Hello World by Hannah Fry\n\n\n\nbook review\n\n\n\n\nApril 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplainer: Building A Modeling Pipeline in PyTorch\n\n\nA Step-by-Step Guide\n\n\n\ndeep learning\n\n\npytorch\n\n\n\n\nApril 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFast.ai course\n\n\nVarious notes for the fast.ai course on deep learning for coders\n\n\n\ncourses\n\n\nnotes\n\n\n\n\nMarch 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Greek tragedy in the 20th century\n\n\nBook Review: Losing Earth by Nathaniel Rich\n\n\n\nbook review\n\n\n\n\nFebruary 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA pinch of AI?\n\n\nBook Review: Human Compatible by Stuart Russell\n\n\n\nbook review\n\n\n\n\nJanuary 11, 2020\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/3-fastai-course-notes/index.html",
    "href": "posts/3-fastai-course-notes/index.html",
    "title": "Fast.ai course",
    "section": "",
    "text": "In this blog post, I would like to share my notes as I go through the course. The course is based on the book Deep Learning for Coders with fastai and PyTorch. Since the course does not include all the chapters of the book, I organized the notes and resources gathered from this course based on the book layout. I will update this post as I create new material.\nCourse information:"
  },
  {
    "objectID": "posts/3-fastai-course-notes/index.html#chapter-1---your-deep-learning-journey",
    "href": "posts/3-fastai-course-notes/index.html#chapter-1---your-deep-learning-journey",
    "title": "Fast.ai course",
    "section": "Chapter 1 - Your Deep Learning Journey",
    "text": "Chapter 1 - Your Deep Learning Journey\n\nLecture 1 - Getting started video\nVisual Notes"
  },
  {
    "objectID": "posts/3-fastai-course-notes/index.html#chapter-2---from-model-to-production",
    "href": "posts/3-fastai-course-notes/index.html#chapter-2---from-model-to-production",
    "title": "Fast.ai course",
    "section": "Chapter 2 - From Model to Production",
    "text": "Chapter 2 - From Model to Production\n\nLecture 2 - Deployment video\nVisual Notes"
  },
  {
    "objectID": "posts/3-fastai-course-notes/index.html#chapter-3---data-ethics",
    "href": "posts/3-fastai-course-notes/index.html#chapter-3---data-ethics",
    "title": "Fast.ai course",
    "section": "Chapter 3 - Data Ethics",
    "text": "Chapter 3 - Data Ethics\nLecture 9 - Data Ethics"
  },
  {
    "objectID": "posts/3-fastai-course-notes/index.html#chapter-4---under-the-hood-training-a-digit-classifier",
    "href": "posts/3-fastai-course-notes/index.html#chapter-4---under-the-hood-training-a-digit-classifier",
    "title": "Fast.ai course",
    "section": "Chapter 4 - Under the Hood: Training a Digit Classifier",
    "text": "Chapter 4 - Under the Hood: Training a Digit Classifier\n\nLecture 3 - Neural net foundations video\nQuestionnaire solutions ch4 (pdf)"
  },
  {
    "objectID": "posts/3-fastai-course-notes/index.html#chapter-5---image-classification",
    "href": "posts/3-fastai-course-notes/index.html#chapter-5---image-classification",
    "title": "Fast.ai course",
    "section": "Chapter 5 - Image Classification",
    "text": "Chapter 5 - Image Classification\n\nQuestionnaire solutions ch5 (pdf)"
  },
  {
    "objectID": "posts/2024-02-11-BR-creativity-code/index.html",
    "href": "posts/2024-02-11-BR-creativity-code/index.html",
    "title": "The AI Muse: Navigating the Complexities of Creativity in the Digital Age",
    "section": "",
    "text": "The Creativity Code is an engaging, lively, and a compelling exploration of the intersection of human creativity and artificial intelligence (AI). The book is focused on several fields balancing philosophical and ethical implications of AI in the creative domain.\nOne disruption that made headlines instantly was a defeat of a world champion by a computer in the game of Go, a popular strategy game once thought to be beyond the realm of computerization. Quite a lot of time is dedicated to discussions on what is creativity in art, what makes it valuable and how social acceptance plays a role, as illustrated by Van Gogh’s posthumous fame despite its lifetime poverty (he was able to sell only two paintings).\nSimilarly, the discussion extends to music, particularly classical, pondering whether musicians are solely creative or they follow some mathematical rules (at least to some point), which would also go in line that computers are good at creating music, which oftentimes is not recognizable by human music professionals.\nFurthermore, it raises questions about the future of mathematicians as the mathematical proofs are becoming so complex that only computers might understand or verify them.\nWritten in a very fluid and inviting style, the book is an easy ready, where you will be surprised as to how much information you retained after reading. Topics are very well connected. The author presents many pondering questions on creativity vs. value, ethical dilemmas in each domain as well as fear of human insignificance due to the computer being able and in many cases more efficient. However, the author also points the benefits of human and AI collaborative work."
  },
  {
    "objectID": "posts/2024-02-11-BR-creativity-code/index.html#references",
    "href": "posts/2024-02-11-BR-creativity-code/index.html#references",
    "title": "The AI Muse: Navigating the Complexities of Creativity in the Digital Age",
    "section": "References",
    "text": "References\nThe banner image for this post was created by OpenAI’s DALL-E."
  },
  {
    "objectID": "posts/2023-04-06-BR-hello-world/index.html",
    "href": "posts/2023-04-06-BR-hello-world/index.html",
    "title": "Unpacking the Implications of Algorithms on Human Life",
    "section": "",
    "text": "Buckle up for an engaging and, at times, unnerving journey as Hannah Fry takes you on a multidisciplinary exploration of how machines and algorithms affect our lives. Be prepared to be amazed and shocked simultaneously as the author presents lucid examples of the applications of algorithms in sensitive areas such as power, data, justice, medicine, cars, crime, and art.\nSummarizing each chapter very briefly:\n\nPower - our ability to trust ourselves, the machine and to over-rule it at times\nData - how much is our personal data worth in the open world system; without precise means to regulate it or even knowing how it is processed\nJustice - no one is perfect, neither the judge nor the algorithm, but they impact our lives significantly, at times, in a biased way feeding even more bias to the initial conditions of both the judge and the algorithm.\nMedicine - the impact of algorithms that can read images\nCars - it turns out it is not that easy to drive a car, let alone make the machine do it, due to numerous obstacles that can occur on and off-road\nArt - it’s hard to predict the success of a movie; is it art if the machine draws it?\n\nThe book does not delve into the mathematical and technical complexities of algorithms. Instead, it focuses only on their implications in real life. The very basics of the types of algorithms are presented, with the more specific machine learning or deep learning mentioned within specific examples and areas. The book focuses more on the human ability or better inability to create algorithms without bias and ethical concerns in mind, finally adequately placing them as active contributors to human society. The author presents numerous vivid examples, at times dire to humans, of how humans have failed to design the process of machine involvement in human life properly.\nThe writing style is engaging and lucid, with “down to Earth” sentences leaving the technical jargon on the library shelf. Due to the style, you will probably read the book in several days (even with note-taking). Furthermore, the language of the writer connects with the reader.\nThe only aspect I would love to see more of is some potential proposal ideas of how to fix these numerous issues we have. But, overall, I think the author does a great job of reaching a very wide audience to raise awareness of the ethical considerations we face.\n\nReferences\nFry, H. (2018). Hello World: How to be Human in the Age of the Machine. Penguin Random House, UK.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2023-04-03-E-model-pipeline-pytorch/index.html",
    "href": "posts/2023-04-03-E-model-pipeline-pytorch/index.html",
    "title": "Explainer: Building A Modeling Pipeline in PyTorch",
    "section": "",
    "text": "This notebook follows a step-by-step process of training a neural network in PyTorch. The objective is on learning the main processes and steps in constructing the modeling pipeline. We will consider a computer vision classification problem, i.e. using a deep learning model to classify images in predefined categories.\nWhen building a deep learning model there are five main things you should consider, namely: - data availability and structure: construct a dataloader for each dataset - model architecture: define a model class - define a modeling pipeline with: - hyperparameters - loss function and the optimizers - model metrics\nConsider the following figure below where the central point is the training and validation box to which we feed the information from our data, the model architecture, etc. to obtain the trained model with its results in the form of a model loss and metrics. This trained model, we can then use to further analyze the resutls on the test set and if all goes well, implement it in production.\nFigure 1. Overview of the components of deep learning modeling pipeline\nSince this is our first step in training a neural network in PyTorch we will focus on constructing the modeling pipeline, i.e. the training and validation box in the above figure, which we can then use in other more complex problems. For this reason we will use the MNIST dataset provided in PyTorch. In the next notebook, we will explore further by using a more complex dataset to demonstrate the importance of constructing a custom dataset and related topics.\n::: {#cell-2 .cell _kg_hide-input=‘false’ execution=‘{“iopub.execute_input”:“2023-04-04T18:00:03.187493Z”,“iopub.status.busy”:“2023-04-04T18:00:03.187057Z”,“iopub.status.idle”:“2023-04-04T18:00:04.701778Z”,“shell.execute_reply”:“2023-04-04T18:00:04.700608Z”,“shell.execute_reply.started”:“2023-04-04T18:00:03.187429Z”}’ trusted=‘true’ execution_count=3}\n:::\nAfter imports we define plot style and global variables. The purpose of global variables are such that they don’t change during the course of the notebook, which is why we define them at the beginning of the notebook.\n# Plotting style\nplt.style.use('ggplot')\n\n# Global variables\nDATA_DIR = Path('/kaggle/working/data/')\nMODELS_DIR = Path('/kaggle/working/models/')\nMODELS_DIR.mkdir(parents=True, exist_ok=True)\nSEED = 42\n\n# Define custom colors for the labels\ncolors = [\"#D9D2D8\",\"#F2BBC9\", \"#BF849A\", \"#8C7A89\", \"#9AC7D9\", \n          \"#82C0D9\", \"#7DABB3\", \"#8F9FBF\", \"#737F99\", \"#566073\"]\ncmap = mcolors.ListedColormap(colors)"
  },
  {
    "objectID": "posts/2023-04-03-E-model-pipeline-pytorch/index.html#prepare-dataset-for-training-create-dataloaders",
    "href": "posts/2023-04-03-E-model-pipeline-pytorch/index.html#prepare-dataset-for-training-create-dataloaders",
    "title": "Explainer: Building A Modeling Pipeline in PyTorch",
    "section": "Prepare dataset for training: Create dataloaders",
    "text": "Prepare dataset for training: Create dataloaders\nPreviously we have downloaded two datasets: training and testing. To be able to deduce whether our model is overfitting during training we need another dataset, namely, validation set. The validation set will show us how our model performs on out-of-sample data, i.e. data that have not been used in training.\nWe will generate the validation dataset by randomly sampling from the training dataset since it is much bigger than the given test set, naturally.\nFor this problem, we can assume that the images in the dataset are independent and don’t have any underlying relationship structure, like time. If this is not the case we should think about the underlying structure when splitting the dataset (more on this in later notebooks). The main aspect we should worry about is the distribution of digits in all datasets. Ideally, we want the digits to be distributed in the same way in all datasets, so when creating the dataset for validation we should keep this in mind.\nFor example, we may encounter two types of imbalance: training imbalance and testing imbalance. In training imbalance, the classes represented are not uniformly distributed, i.e. there is a significant perentage of one or several classes compared to the rest of the classes in the dataset. On the other hand, testing imbalance refers to the imbalance between training and testing dataset. For example, if we would have a training set with an extremly small number of say digits 3 but a large sample of digits 3 in the test set then the model has a very limited information scope to learn from, but it is expected to know 3s very well in out-of-sample settings. You can notice that in this case there is most probably evidence of both imbalance problems.\nTo begin, it is usefull to know how the labels are named. We can use class_to_idx to get the dictionary of labels in our dataset.\n\n# Get mapping of target IDs\nprint(data_train.class_to_idx)\n\n{'0 - zero': 0, '1 - one': 1, '2 - two': 2, '3 - three': 3, '4 - four': 4, '5 - five': 5, '6 - six': 6, '7 - seven': 7, '8 - eight': 8, '9 - nine': 9}\n\n\nTo check the distributions of labels in our training and testing dataset visually we compute a bar plot of each:\n::: {#cell-25 .cell _kg_hide-input=‘false’ execution=‘{“iopub.execute_input”:“2023-04-04T18:00:05.328806Z”,“iopub.status.busy”:“2023-04-04T18:00:05.328446Z”,“iopub.status.idle”:“2023-04-04T18:00:05.658155Z”,“shell.execute_reply”:“2023-04-04T18:00:05.657329Z”,“shell.execute_reply.started”:“2023-04-04T18:00:05.328768Z”}’ trusted=‘true’ execution_count=14}\n\nCode\n# Create dataframe for each dataset\ndf = pd.DataFrame(pd.Series(data_train.targets).value_counts().reset_index())\ndf.columns = ['digit', 'count']\ndf.sort_values('digit', inplace=True)\ndft = pd.DataFrame(pd.Series(data_test.targets).value_counts().reset_index())\ndft.columns = ['digit', 'count']\ndft.sort_values('digit', inplace=True)\n\n# Define plot structure\nfig,_=plt.subplots(1,2, figsize=(10,3))\nplt.suptitle('Target distribution label', size=15)\n\n# Plot the train set\nplt.subplot(1,2,1)\nplt.bar('digit', 'count',data=df, color=cmap(df.digit))\nplt.title('Train set', size=10)\nplt.xlabel('digit')\nplt.xticks(range(len(df.digit)), df.digit)\nplt.ylabel('count')\n\n# Plot the test set\nplt.subplot(1,2,2)\nplt.bar('digit', 'count',data=dft, color=cmap(dft.digit))\nplt.title('Test set', size=10)\nplt.xlabel('digit')\nplt.xticks(range(len(dft.digit)), dft.digit)\n\nplt.show()\n\n\n\n\n\n\n\n\n:::\nWe can see from above figures that the target distributions for both training and test sets are similar and all digits are represented relatively in a balanced structure, which means we can just split the training data randomly into training and validation sets. As noted previously if the sample is not balanced then when splitting the sample we should preserve the sample imblance.\nWe will use SubsetRandomSampler to select the data. The SubsetRandomSampler uses indices from the original dataset to randomly and without replacement select subsets of data. So, first we need to define which indices are for the training sample and which are to be used for the validation sample. Generally, if there is no underlying structure in the data, i.e. the samples are independent, it is good practice to shuffle the data before selecting the indices. This ensures that we will have samples from the complete set of data. Note that the function SubsetRandomSampler selects random indices from the given list, however, we need to first define from which list to select these samples from.\nWe will construct the training set to be 70% of the indices of the original downloaded training dataset. The remaining 30% will be allocated to validation dataset. The steps are:\n\n# Step 1 - Length of train dataset from which we are splitting the data\nNd = len(data_train); print(f'Dataset length: {Nd}')\n\n# Step 2 - Create a shuffled list of training indices to ensure\n# there are indices from the complete set in the final selection\nNd_idx = list(range(Nd))\nnp.random.shuffle(Nd_idx)\nprint(f'Shuffled indices {Nd_idx[:5]}')\n\n# Step 3 - Define percentage of indices for the training sample\n# to compute number of indices to be included in the training sample\ntrain_pct = 0.7\nNt = int(np.floor(train_pct * Nd))\nprint(f'Number of indices to include in training set: {Nt}')\n\n# Step 4 - Split the indices into training and validation\ntr_idx, vl_idx = Nd_idx[:Nt], Nd_idx[Nt:]\n\n# Pass the indices to the SubsetRandomSampler\ntr_sampler = SubsetRandomSampler(tr_idx)\nvl_sampler = SubsetRandomSampler(vl_idx)\n\nprint(f'\\nTrain idx length: {len(tr_sampler)}')\nprint(list(tr_sampler)[:5])\nprint(f'\\nValid idx length: {len(vl_sampler)}')\nprint(list(vl_sampler)[:5])\n\nDataset length: 60000\nShuffled indices [8939, 47442, 57909, 43484, 27583]\nNumber of indices to include in training set: 42000\n\nTrain idx length: 42000\n[16519, 23559, 32979, 55729, 15783]\n\nValid idx length: 18000\n[32198, 34233, 43322, 23625, 1148]\n\n\nNow, that we have the samplers for training and validation, we can create iterable objects for each dataset which will contain all the information used in the training process with respect to the input data. These objects are called dataloaders.\nIn order to train the model using PyTorch there are two basic requirements in regards to the input data: 1. the dataset has to be in the form of a tuple with the structure (input, label) where each is a tensor 2. the input data should be stored in batches, i.e. the input data is iterable over batches.\nNote that the initial data is already in form of tuples, as we saw previously. To create the iterable batches we will use a PyTorch object called DataLoader which takes a Python collection and converts it to an iterator based on batches. From the PyTorch documentation we have:\n\nData loader. Combines a dataset and a sampler, and provides an iterable over the given dataset. Ref\n\nSince we have datasets and the corresponding samplers we can proceed to create dataloaders, but before actually constructing dataloaders let’s check what we have got in training and validation datasets. We also need to check whether there are any partial batches, i.e. leftover samples when constructing mini-batches. Note that there is an option in DataLoader called drop_last to drop any such letfovers. In the function below we are computing, based on our defined samplers, how many batches we should expect from the datasets and whether there are letfover samples, i.e. incomplete batches.\n\n# Define number of batches\nno_batches = 64\n\n\ndef check_batches(sampler, batch_size):\n    \n    # Number of samples to expect in the final dataloader\n    no_samples = len(sampler)\n    \n    # Number of batches to expect in the final dataloader\n    no_batches = int(np.ceil(no_samples / batch_size))\n    print(f'Number of batches total: {no_batches}')\n    \n    # Samples in the last batch (leftover samples)\n    lb_samples = no_samples % batch_size\n    \n    if lb_samples != 0:\n        no_batches -= 1\n    \n    print(f'Full batches: {no_batches}')\n    if lb_samples != 0:\n        print(f'Samples in partial batch: {lb_samples}')\n    \n    return no_batches, lb_samples\n\nWe can apply the above function on training and validation sample:\n\nprint('Training sample')\ntr_batches, tr_lb_samples = check_batches(tr_sampler, no_batches)\nprint('\\nValidation sample')\nvl_batches, vl_lb_samples = check_batches(vl_sampler, no_batches)\n\nTraining sample\nNumber of batches total: 657\nFull batches: 656\nSamples in partial batch: 16\n\nValidation sample\nNumber of batches total: 282\nFull batches: 281\nSamples in partial batch: 16\n\n\nNow, let’s finally create dataloaders. Note that we can not use shuffle=True when using SubsetRandomSampler() by construction.\n\n# Training dataloader\ndl_train = torch.utils.data.DataLoader(data_train, batch_size=no_batches, \n                                       shuffle=False, sampler=tr_sampler)\n\n# Validation dataloader\ndl_valid = torch.utils.data.DataLoader(data_train, batch_size=no_batches, \n                                       shuffle=False, sampler=vl_sampler)\n\n# Test dataloader\ndl_test = torch.utils.data.DataLoader(data_test, batch_size=no_batches, \n                                      shuffle=True)\n\nTo check the size of the dataloader we can use len() which will give us the number of batches created for each dataset. The numbers for the batches from the dataloaders align with what we have calculated from the samplers.\n\nprint(f'Training batches: {len(dl_train)}')\nprint(f'Validation batches: {len(dl_valid)}')\nprint(f'Test batches: {len(dl_test)}')\n\nTraining batches: 657\nValidation batches: 282\nTest batches: 157\n\n\n\n\n\n\n\n\nNote\n\n\n\nSince we have constructed training and validation dataloaders from the original dataset using sampler, if we call len(dl_train.dataset) it will give us the number of samples of the original dataset, i.e. 60000, and not 42000 and 18000 respectively. We will see how this applies later on in the code when we will compute the average loss and metric for the epoch of training.\n\n\n\nprint(len(dl_train.dataset), len(dl_valid.dataset))\n\n60000 60000\n\n\nFinally let’s check the shape of one batch:\n\nfor batch in dl_train:\n    X, y = batch\n    print(f'X shape: {X.shape}')\n    print(f'y shape: {y.shape}')\n    break\n\nX shape: torch.Size([64, 1, 28, 28])\ny shape: torch.Size([64])\n\n\nSo, everything looks good, we have batches with the correct number of samples and the shapes of each sample is 28 by 28 with 1 channel since the images were given in gray scale and not in RGB (then we would have 3 channels instead of 1)."
  },
  {
    "objectID": "posts/2023-04-03-E-model-pipeline-pytorch/index.html#the-loss-function",
    "href": "posts/2023-04-03-E-model-pipeline-pytorch/index.html#the-loss-function",
    "title": "Explainer: Building A Modeling Pipeline in PyTorch",
    "section": "The Loss Function",
    "text": "The Loss Function\nLoss function provides the connection between our model predictions and the ground truth (the target labels) in form of a measure, which tells us how far our model predictions are from the target labels. It serves as a means to verify if our optimization process of the weights is progressing as intended, i.e. our model is making better (more correct) predictions as we iterate the learning process.\nThe loss function needs to have certain properties to be useful in model training: 1. differentiable - if the loss function is not differentiable there are no gradients which would update the weights, without updating the weights there can be no change in the model predictions 2. sensitive - responds to small changes in weights which in turn means that it will change the prediction value. If there is no change in the prediction value the training iteration is useless. 3. It can be the same as model metric only if it satisfies the first 2 properties. For example, accuracy is a common metric (we will use it in this project) however, it is not suitable as a loss function since it is not differentiable and it does not possess the properties of sensitivity.\nIn this project the problem is of multi-class classification, and we should choose the loss function which will satisfy the above properties and provide us with the probabilities for each underlying class of labels. In most cases, cross entropy loss function is used.\nCross-entropy loss function is made of two components: 1. the softmax activation function, and - provides prediction probabilities for each class which sum to 1 2. the negative log likelihood - since we are transforming the values from (0,1) by means of taking the log the output range is then (log(0), log(1)) = (-inf, 0) and we need to multiply with (-1) to get the positive loss values.\nThe cross-entropy loss is defined in PyTorch as follows:\n# Instantiate the loss function\nloss_fun = nn.CrossEntropyLoss()\n\n# Compute the loss\nloss = loss_fun(inputs, labels)"
  },
  {
    "objectID": "posts/2023-04-03-E-model-pipeline-pytorch/index.html#optimizer",
    "href": "posts/2023-04-03-E-model-pipeline-pytorch/index.html#optimizer",
    "title": "Explainer: Building A Modeling Pipeline in PyTorch",
    "section": "Optimizer",
    "text": "Optimizer\nThe loss function gave us the information how far our model’s outputs are from the labels. The next step is to try to minimize this loss, by adjusting the model parameters, for which we need two things: the direction to the potential minimum loss and the path to get there. The direction is given by the gradient of the loss function with respect to the parameters, while the path is given by the optimizer. The speed at which we would like to trod along this path is provided by the hyperparameter called learning rate (more on this in the next notebooks).\nOptimizer is\nIn the figure below the process of optimizing model parameters is given:\n Figure 2. Process of updating the model parameters\n\ninitialize the weights - use random values\nfor each training sample in a mini-batch use the weights to compute the prediction\ngiven the calculated predictions compute the model loss on the mini-batch\ncalculate the gradients of the loss with respect to weight parameters - tells us the direction of the loss for one unit change in the parameters\nupdate the weights according to step (4)\nrepeat the steps from (2) to (5)\niterate until the model loss is low enough and the model is not overfitting or there is a time constraint\n\nThere are many optimizers to choose from like gradient descent (GD), stochastic gradient descent (SGD), Adagrad, RMSprop, Adam, etc. where GD and SGD is the first one you will most likely encounter in stuidying deep learning. For the purposes of this notebook we will use SGD and not dwelve into the details of a particular optimizers, this we leave for future notebook explainers. In PyTorch we can define it as follows:\noptimizer = torch.optim.SGD(model.parameters(), lr=lr_rate)"
  },
  {
    "objectID": "posts/2023-04-03-E-model-pipeline-pytorch/index.html#metric",
    "href": "posts/2023-04-03-E-model-pipeline-pytorch/index.html#metric",
    "title": "Explainer: Building A Modeling Pipeline in PyTorch",
    "section": "Metric",
    "text": "Metric\nThe main goal of a machine learning model is to generalize well on unseen data. To assess whether we have achieved this we need an independent view of our model performance. Computing the model metric on the out-of-sample (validation dataset) provides us with such information.\nDuring training the weights are updated using the training datset given the loss function, gradients and the optimizer. So in essence, we use training set to update (iteratively) the weights and the validation set to assess model generalization.\nModel metric is given in terms of explainability of the model performance given the project goals. The metric needs to be “human understandable” while the loss function needs to be suitable mathematically for the optimization process (SGD).\nThere may be cases where the loss function and metric are equally defined.\nIn this project, we want to see how many images are classified correctly so we can use a simple accuracy metric to acomplish this objective. There is no direct function in PyTorch for accuracy, so we define it directly within the training process with the following steps: 1. convert from model outputs to class labels by selecting the class with the highest output 2. compare model class output to labels and sum the correct predictions 3. divide the sum of correct predictions with the total number of samples"
  },
  {
    "objectID": "posts/2023-04-03-E-model-pipeline-pytorch/index.html#hyperparameters",
    "href": "posts/2023-04-03-E-model-pipeline-pytorch/index.html#hyperparameters",
    "title": "Explainer: Building A Modeling Pipeline in PyTorch",
    "section": "Hyperparameters",
    "text": "Hyperparameters\nHyperparameters are parameters that are defined before we start training and do not change during training, i.e. they are the parameters that are not updated. They should not be confused with weights and biases, which are trainable model parameters and are updated, i.e. optimized during training.\nExamples of hyperparameters include: - batch size - number of epochs - learning rate - regularization - weight initialization\nFor this project, we will define the first three hyperparameters. The batch size we have already considered when defining the dataloaders, while the number of epochs and the learning rate is defined prior to training.\nNote that as you change any of the hyperparameters the results of your deep learning model will change. For example, changing the learning rate has a direct effect on how fast the model converges (or doesn’t) to a solution (it might not be optimal). Finding the best hyperparameters for your project is one of the key components of deep learning, i.e. achieving optimal performance on a given task.\nSince this is a first introductory notebook we will use only one value for the batch size, number of epochs, and learning rate. In later notebooks, we will explore the effects of the hyperparamters on the model results."
  },
  {
    "objectID": "posts/2023-04-03-E-model-pipeline-pytorch/index.html#model-architecture",
    "href": "posts/2023-04-03-E-model-pipeline-pytorch/index.html#model-architecture",
    "title": "Explainer: Building A Modeling Pipeline in PyTorch",
    "section": "Model Architecture",
    "text": "Model Architecture\nModel architecture provides the functional form of the model. It specifies how the input samples are passed through the collection of mathematical functions to obtain the final prediction value. Model architecture is comprised of layers, namely the input layer, hidden layers and the output layer. It is the stucture of the hidden layers that leads to deep networks, i.e. the more hidden layers the deeper the network. Model architecture, as you could imagine, has a direct impact on the performance of our model. With bigger (deeper) architecture we can expect better model performance, however with some cavetas. We can explore these in the future notebooks. For now we will consider a simple linear neural network to illustrate the process of defining the model architecture. As mentioned earlier, the goal of this notebook is not to train the best possible model, but to explain the modeling pipeline, which can then be tuned to develop a much better performing model.\nA simple neural network with linear layers will provide a fast baseline to check that our modeling pipeline works. So first, we define the class for model architecture by inheriting from the PyTorch nn.Module, which is the base class for all neural network models.\nWithin our model class we need to define the required __init__ and forward methods. In order to invoke thenn.Module we need to add super().__init__() within the __init__() method. This ensures that SimpleLNN inherits all the basic functionality of nn.Module and it first executes the code in the parent’s class i.e. in nn.Module. The forward method defines how the data will pass through the defined network.\nLet’s define the class SimpleLNN:\n\nclass SimpleLNN(nn.Module):\n    \"\"\"\n    A simple linear neural network with 2 linear layers\n    and an output layer with softmax activation function.\n    \n    in_shape (tuple): (height, width)\n    n_out (int): number of outputs of the model\n    \"\"\"\n    \n    def __init__(self, in_shape:tuple, n_out:int):\n        super().__init__()\n        torch.manual_seed(1)\n        np.random.seed(1)\n        self.in_shape = in_shape\n        H, W = self.in_shape\n        self.n_out = n_out\n        self.flatten = nn.Flatten()\n        self.linear = nn.Linear(H*W, self.n_out)\n        \n    def forward(self, x):\n        x = self.flatten(x)\n        x = self.linear(x)\n        \n        return x\n\nGiven our simple neural network let’s see how many parameters we have to train per each layer. We define a simple function ourselves as follows:\n\ndef cnt_params(model, show_per_layer:bool=True):\n    \"\"\"\n    Get the number of model parameters for the instantiated model class.\n    If show_per_layer then print info for each layer.\n    \"\"\"\n\n    if show_per_layer:\n        print('-'*75)\n        print(f'PARAMETER INFORMATION PER LAYER')\n        print('•'*75)\n        for name, param in model.named_parameters():\n            if param.ndim &lt; 2: \n                in_fts = param.ndim\n            else:\n                in_fts = param.shape[1]\n            out_fts = param.shape[0]\n            print(f\"Layer: {name}  | In Params: {in_fts}  | Out Params: {out_fts}  |  Total Params: {in_fts*out_fts}\")\n    \n    total_cnt_params = sum([x.reshape(-1).shape[0] for x in model.parameters()])\n    print('-'*75)\n    print(f'Total number of parameter: {total_cnt_params}')\n    print('-'*75)\n\n    return total_cnt_params\n\n\nmodel = SimpleLNN((28,28), 10)\n_ = cnt_params(model)\n\n---------------------------------------------------------------------------\nPARAMETER INFORMATION PER LAYER\n•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••\nLayer: linear.weight  | In Params: 784  | Out Params: 10  |  Total Params: 7840\nLayer: linear.bias  | In Params: 1  | Out Params: 10  |  Total Params: 10\n---------------------------------------------------------------------------\nTotal number of parameter: 7850\n---------------------------------------------------------------------------"
  },
  {
    "objectID": "posts/2020-02-15-Losing-Earth/index.html",
    "href": "posts/2020-02-15-Losing-Earth/index.html",
    "title": "A Greek tragedy in the 20th century",
    "section": "",
    "text": "Last year I read the Uninhabitable Earth by David Wallace-Wells, which halfway through felt like an ice shower being on a circular loop. By the time I reached the end of the book, I felt so horrible for all the children and the “gift” we are so proudly giving, the wrecked planet. What an inheritance! I bet none of them would love us more for it. Then I picked up this book, Losing Earth: The Decade We Could Have Stopped Climate Change, which judging by the title will provide for yet another icy shower. Long story short, it did not disappoint.\nStarting strong with the first sentence, “Nearly everything we understand about global warming was understood in 1979.”, meaning we have been incompetent for the past 40 years! I say incompetent since we had all the information needed to start acting on it. Sadly, some are still applauding themselves.\nThis science history book reads like a thriller but feels like a Greek tragedy, with the plot, riding the structural wave with twists and turns showcasing the battle between the good and the evil. If only we didn’t know how this scenario would end.\nLosing Earth provides a detailed historical account of the many events, House and Senate hearings, reports, conferences, that shaped the climate “debate” starting in 1979 and leading up to the Rio Earth Summit in 1992. We follow the journey of the lead character, an environmental lobbyist, Rafe Promenance, and a NASA scientist Jim Hansen, who each in their way battle with the political circle, oil&gas industry, and the general public. Their primary goal is the acknowledgment of the scientific results on the greenhouse effects and the first official actions of a solution. This battle, at times, does not end well for them, as you could imagine.\nWhat to do? It is discouraging to think that with all the information, technology, and advancements we pride ourselves, we are still mostly blind and unable to move past the crawling stage of human development and start walking into a better future.\nTo end the review, I will borrow a quote from dr. Tyson’s latest book The Letters from An Astrophysicist on the question “What if you are President” to which he answers:\n\n“One objective reality is that our government doesn’t work, not because we have dysfunctional politicians, but because we have dysfunctional voters. As a scientist and educator, my goal, then, is not to become President and lead a dysfunctional electorate, but to enlighten the electorate so they might choose the right leaders in the first place.”\n\n\nReferences\nRich, Nathaniel (2020). Losing Earth: A Climate History. PICADOR.\nTyson, N. D. G. (2019). Letters from an astrophysicist. New York: W. W. Norton & Company.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2020-01-11-human-compatible/index.html",
    "href": "posts/2020-01-11-human-compatible/index.html",
    "title": "A pinch of AI?",
    "section": "",
    "text": "The research in AI is growing without the rules or regulations, but with the common goal of achieving the general purpose AI. Do you notice the problem? With many notable achievements in the field, the AI revolution started with many jumping on the high-speed train. It seems as if any project somehow sounds and looks better if we add a pinch of AI to it. But does it? Are we driving blindly? Most likely. Some researchers already ceased their research efforts because the results and methods are being used in a way that is not beneficial to society. If you are one of the passengers on the high-speed train, please take the blindfold off and read this book to be able to act accordingly before your research goes a step too far.\nHuman Compatible is an honest, eye-opening account of the AI potential and the right way forward, its current shortcomings, and, if we choose the wrong path, our doom. The question is, how do we create an AI system so that we don’t need to consider the control issue, or lack of control for that matter, in every scenario of AI application? How do we overcome the problem of the AI system being willing to be shut down?\nRussell takes us back in history to revisit the routes taken in human learning, decision making, and actions taken, as well as how we transferred the learning process onto a machine. As we know, many applications have the AI label from speech recognition, vision systems, translation, etc. and many more will come. But our overall mission is to develop the so-called general-purpose AI, which by Russell is “…a method that is applicable across all problem types and works effectively for large and difficult instances while making very few assumptions.” Sounds good, but as he notes in many instances across the book, many precautions need to be taken along the way. For example, how to align AI and human objectives, how to incorporate our uncertainties and irrational decisions? In short, we need algorithms that would show that the end goal is beneficial to humans.\nIf you want a fresh perspective on what constitutes AI, how to adjust our current practices, or the standard model for an AI, more in line with our uncertain preferences and objectives; then, this book will help you in this quest. Stuart Russell, one of the most prominent persons in the AI world, embarks on the mission to not only examine how and why the AI could go wrong but most importantly what to consider and how to proceed so that we, the humanity, don’t end up on the flimsy and half-broken tree branch of existence.\nIn many science disciplines, we battle with the question of how our reality will cease to exist. In astronomy, it is in the way of the Sun engulfing the Earth, in biology, it might be some deadly virus that spreads quickly and efficiently, in environmental science it is the climate change, in computer science, it is the AI. Due to the laws of physics, we can’t much argue with the Sun engulfing the Earth scenario, but with the AI, we can change the dooms outcome since we are the ones creating it and its laws. In Human Compatible, Russell provides an eye-opening perspective on how to mitigate the AI risk and achieve the human compatible status. The secret lies in how we define the objectives of an AI system and its ability or understanding of the need to be switched off at any time that the human deems essential.\nI enjoyed the book, which makes for a smooth and light read due to the elegant, flowy, and transparent presentation of ideas and concepts despite the complexity of the topic. The voice is loud and steady, providing for yet another dimension of the seriousness of the issue. For readers in the AI world, the book does not require much specific pre-knowledge, where the general public may need a bit more context, aside from the Appendix provided, to understand the reasons for the whole gloom and dusk scenarios.\n\nQuotes\n\nHumans are intelligent to the extent that our actions can be expected to achieve our objectives.\n\n\nMachines are beneficial to the extent that their actions can be expected to achieve our objectives.\n\n\n\nReferences\nRussell, S. (2019). Human Compatible: AI and the Problem of Control. Allen Lane, Penguin Random House, UK.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Most of my time is spent finding stories in data and developing predictive models using machine learning methods. In my free time, I’m usually in the bookstores browsing for the next exciting adventure read or in some cozy space reading books. I also love to cook; you can check my cooking adventures in my foodieland world.\n\n\n Back to top"
  }
]